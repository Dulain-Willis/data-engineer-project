x-airflow-common: &airflow-common
  #shared config for airflow services
  image: airflow-custom
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://postgres:postgres@postgres:5432/postgres
    AIRFLOW_UID: "50000"
    AIRFLOW_CONN_SPARK_DEFAULT: spark://spark-master:7077
  depends_on:
    - postgres
  volumes:
    - ./dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./spark_jobs:/opt/airflow/spark_jobs

services:

  postgres:
  #Airflow DB
    image: postgres:16
    container_name: postgres
    environment:
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"

  minio:
  #S3 like object/bucket storage
    image: minio/minio
    container_name: minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001" 
    ports:
      - "9000:9000"
      - "9001:9001" 
    volumes:
      - ./.mnt/minio:/data

  clickhouse:
  #Analytics OLAP Warehouse
    image: clickhouse/clickhouse-server
    container_name: clickhouse
    environment:
      CLICKHOUSE_USER: clickhouse
      CLICKHOUSE_PASSWORD: clickhouse
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: "1"
      CLICKHOUSE_ALWAYS_RUN_INITDB_SCRIPTS: "true"
    ports:
      - "8123:8123"   # HTTP interface
      - "9100:9000"   # Native TCP
    volumes:
      - ./.mnt/clickhouse/data:/var/lib/clickhouse/
      - ./.mnt/clickhouse/logs:/var/log/clickhouse-server/
      - ./infra/clickhouse/init:/docker-entrypoint-initdb.d/
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
  
  mc:
  #CLI for interacting with minio
    image: minio/mc
    container_name: mc 
    entrypoint: /bin/bash #docker compose run --rm mc \ to hop in container using mc
    tty: true
    depends_on:
      - minio
    volumes:
      - ./.mnt/mc:/root/.mc

  terraform:
  #IAC for minio buckets
    image: hashicorp/terraform
    container_name: terraform
    working_dir: /workspace
    entrypoint: ["terraform"]
    volumes:
      - ./infra/terraform:/workspace

  spark-master:
    build:
      context: .
      dockerfile: infra/docker/spark/Dockerfile
    container_name: spark-master
    environment:
      - SPARK_NO_DAEMONIZE=true
    env_file:
      - .env
    entrypoint: /bin/bash
    command: -c "/opt/spark/sbin/start-master.sh"
    ports:
      - "8081:8080"
      - "7077:7077"

  spark-worker:
    build:
      context: .
      dockerfile: infra/docker/spark/Dockerfile
    container_name: spark-worker
    environment:
      - SPARK_NO_DAEMONIZE=true
    env_file:
      - .env
    entrypoint: /bin/bash
    command: -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077"
    depends_on:
      - spark-master

  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    command: bash -c "airflow db init && airflow users create --firstname admin --lastname admin --email admin --password admin --username admin --role Admin"
    depends_on:
      - postgres
    restart: "no"

  airflow-webserver:
    build:
      context: .
      dockerfile: infra/docker/airflow/Dockerfile
    container_name: airflow-webserver
    <<: *airflow-common 
    command: webserver
    ports:
      - "8080:8080"
    restart: always

  airflow-scheduler:
    build:
      context: .
      dockerfile: infra/docker/airflow/Dockerfile
    container_name: airflow-scheduler
    <<: *airflow-common 
    command: scheduler
    restart: always

